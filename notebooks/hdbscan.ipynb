{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimpy import skim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "log_dir = Path.cwd().parent / \"logs\"\n",
    "logger.add(log_dir / \"log.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCHES = 250\n",
    "LAST_TRAIN_MATCH_DATE = datetime(2023, 7, 31)\n",
    "this_dir = Path().resolve()\n",
    "DATA_DIR = this_dir.parent / \"data\"\n",
    "print(DATA_DIR)\n",
    "df = pd.read_csv(DATA_DIR / \"ml_rows.csv\", parse_dates=[\"start_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate train/test sets\n",
    "\n",
    "test data is the more recent set of matches - want to see how well the clusters produced match the outcomes from the data used in clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = {\n",
    "    inns: df[(df.start_date <= LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES) & (df.innings == inns)]\n",
    "    for inns in range(2)\n",
    "}\n",
    "df_test = {\n",
    "    inns: df[(df.start_date > LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES) & (df.innings == inns)]\n",
    "    for inns in range(2)\n",
    "}\n",
    "\n",
    "train_dfs = {\n",
    "    inns: {over: df_train[inns].loc[(df_train[inns][\"over\"] == over)] for over in range(20)} for inns in range(2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(d, values: list[str]):\n",
    "    return d[values]\n",
    "\n",
    "\n",
    "X_values = [\n",
    "    \"wickets_down\",\n",
    "    \"run_rate\",\n",
    "    \"req_rate\",\n",
    "    \"batter_in_first_10\",\n",
    "    \"batter_strike_rate\",\n",
    "    \"batter_dismissal_prob\",\n",
    "    \"batter_dismissal_vs_style\",\n",
    "    \"bowler_economy\",\n",
    "    \"bowler_wicket_prob\",\n",
    "    \"bowler_wicket_vs_style\",\n",
    "    \"bowler_wide_noball_rate\",\n",
    "]\n",
    "\n",
    "y_values = [\n",
    "    \"outcome\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER_CLASSES = {\n",
    "    \"quantile\": preprocessing.QuantileTransformer(),\n",
    "    \"standard\": preprocessing.StandardScaler(),\n",
    "    \"gaussian\": preprocessing.QuantileTransformer(output_distribution=\"normal\"),\n",
    "    \"normalizer\": preprocessing.Normalizer(),\n",
    "}\n",
    "\n",
    "\n",
    "def prepare_training_set(inns, over, scaler: str) -> np.ndarray:\n",
    "    X = extract_values(train_dfs[inns][over], X_values)\n",
    "    scaler_obj = SCALER_CLASSES[scaler]\n",
    "    fitted_scaler = scaler_obj.fit(X)\n",
    "    return fitted_scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate most effective numbers of clusters per innings, over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "\n",
    "def test_n(n: int, X) -> float:\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(X)\n",
    "    return kmeans.inertia_\n",
    "\n",
    "\n",
    "def find_knee(inns, over):\n",
    "    X = prepare_training_set(inns, over, \"normalizer\")\n",
    "    clusters_range = range(11, 31, 1)\n",
    "    sse = [test_n(n, X) for n in clusters_range]\n",
    "    kneedle = KneeLocator(clusters_range, sse, curve=\"convex\", direction=\"decreasing\")\n",
    "    return kneedle.knee\n",
    "\n",
    "\n",
    "knees = {}\n",
    "\n",
    "with tqdm(total=40) as pbar:\n",
    "    for inns in range(2):\n",
    "        knees[inns] = []\n",
    "        for over in range(20):\n",
    "            res = find_knee(inns, over)\n",
    "            knees[inns].append(res)\n",
    "            pbar.update()\n",
    "\n",
    "for inns in range(2):\n",
    "    print(inns, \",\".join([f\"{v}\" for v in knees[inns]]))\n",
    "    print(f\"{inns} mean: {np.mean(knees[inns])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build clusters\n",
    "\n",
    "Given a cluster count (in `knees`) fit each over's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from hdbscan import flat\n",
    "\n",
    "clfs: dict[int, list] = {0: [], 1: []}\n",
    "\n",
    "logger.info(\"starting with estimated clusters...\")\n",
    "for scaler in [\n",
    "    \"normalizer\",\n",
    "]:  # [\"standard\", \"quantile\", \"gaussian\", \"normalizer\"]:\n",
    "    for inns in range(2):\n",
    "        for over in range(20):\n",
    "            X_scaled = prepare_training_set(inns, over, scaler)\n",
    "            n_clusters = knees[inns][over]\n",
    "            clf = flat.HDBSCAN_flat(X_scaled, n_clusters=n_clusters)\n",
    "            clf.fit(X_scaled)\n",
    "            clfs[inns].append(clf)\n",
    "            labels = clf.labels_\n",
    "            clustered = labels >= 0\n",
    "            counts = Counter(labels)\n",
    "            total_clusters = np.max(labels) + 1\n",
    "            coverage = np.sum(clustered) / X_scaled.shape[0]\n",
    "            total_clusters = np.max(labels) + 1\n",
    "            logger.info(\n",
    "                f\"{inns} {over} {n_clusters} {total_clusters} {coverage:.2%} pts:{len(labels)} noisy:{counts[-1]} {[v for k, v in counts.items() if k >= 0]}\"\n",
    "            )\n",
    "logger.info(\"all done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine clustering outcomes\n",
    "\n",
    "Run a T-SNE plot for the overs in an innings to get an impression of what kind of results we are getting. Use a sample to reduce execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", font_scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "inns = 0\n",
    "\n",
    "for over in tqdm(range(20)):\n",
    "    data = train_dfs[inns][over].sample(10000)\n",
    "\n",
    "    X = extract_values(data, X_values)\n",
    "    scaler_obj = SCALER_CLASSES[\"normalizer\"]\n",
    "    fitted_scaler = scaler_obj.fit(X)\n",
    "    Xp = fitted_scaler.transform(X)\n",
    "\n",
    "    tsne = TSNE(n_components=2, metric=\"euclidean\", verbose=0, perplexity=40, n_iter=300)\n",
    "    results = tsne.fit_transform(Xp)\n",
    "\n",
    "    n_clusters = knees[inns][over]\n",
    "    # clf = flat.HDBSCAN_flat(X, n_clusters=n_clusters)\n",
    "    clf = clfs[inns][over]\n",
    "    clf.fit(Xp)\n",
    "    clfs[inns].append(clf)\n",
    "    labels = clf.labels_\n",
    "\n",
    "    df_res = pd.DataFrame()\n",
    "    df_res[\"x\"] = results[:, 0]\n",
    "    df_res[\"y\"] = results[:, 1]\n",
    "    df_res[\"c\"] = labels\n",
    "\n",
    "    r = over % 5\n",
    "    c = over // 5\n",
    "    ax = axs[c, r]\n",
    "    sns.scatterplot(\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        data=df_res,\n",
    "        hue=\"c\",\n",
    "        palette=sns.color_palette(\"hls\", 10),\n",
    "        legend=False,\n",
    "        alpha=0.3,\n",
    "        ax=ax,\n",
    "        s=10,\n",
    "    ).set(\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        xlabel=None,\n",
    "        ylabel=None,\n",
    "        title=f\"over {over}\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply clusterings - extract probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inns, over = 0, 8\n",
    "clf = clfs[inns][over]\n",
    "Xt = extract_values(train_dfs[inns][over], X_values)\n",
    "yt = extract_values(train_dfs[inns][over], y_values)\n",
    "scaled_Xt = SCALER_CLASSES[\"normalizer\"].fit(Xt).transform(Xt)\n",
    "clf.fit(scaled_Xt)\n",
    "labels = clf.labels_\n",
    "outcomes = yt[\"outcome\"]\n",
    "outcomes.reset_index(drop=True, inplace=True)\n",
    "print(len(labels), len(outcomes))\n",
    "\n",
    "res = {k: defaultdict(int) for k in np.unique(labels)}\n",
    "for i, label in enumerate(labels):\n",
    "    res[label][outcomes[i]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTCOME_DESCS = [\n",
    "    \"label\",\n",
    "    \"tot\",\n",
    "    \"wide\",\n",
    "    \"noball\",\n",
    "    \"bye\",\n",
    "    \"lebbye\",\n",
    "    \"wicket\",\n",
    "    \"dot\",\n",
    "    \"single\",\n",
    "    \"two\",\n",
    "    \"three\",\n",
    "    \"four\",\n",
    "    \"six\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title():\n",
    "    return f\"## Innings {inns}, Over {over}\"\n",
    "\n",
    "\n",
    "def table_hdr():\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            \"| \" + \" | \".join(OUTCOME_DESCS) + \" |\",\n",
    "            \"| \" + \" | \".join(\"---:\" for _ in OUTCOME_DESCS) + \" |\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def table_line(k, v):\n",
    "    tot = sum(v.values())\n",
    "    return (\n",
    "        f\"| {k:-2d} | {tot:-6d} | \"\n",
    "        + \" | \".join(f\"{v[i] / tot:8.2%}\" for i in range(np.unique(outcomes).shape[0]))\n",
    "        + \" |\"\n",
    "    )\n",
    "\n",
    "\n",
    "body_lines = \"\\n\".join(table_line(k, v) for k, v in res.items())\n",
    "\n",
    "display(Markdown(title() + \"\\n\" + table_hdr() + \"\\n\" + body_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
