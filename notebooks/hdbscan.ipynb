{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import hdbscan\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "log_dir = Path.cwd().parent / \"logs\"\n",
    "logger.add(log_dir / \"log.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCHES = 250\n",
    "LAST_TRAIN_MATCH_DATE = datetime(2023, 7, 31)\n",
    "this_dir = Path().resolve()\n",
    "DATA_DIR = this_dir.parent / \"data\"\n",
    "print(DATA_DIR)\n",
    "df = pd.read_csv(DATA_DIR / \"ml_rows.csv\", parse_dates=[\"start_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = {\n",
    "    inns: df[(df.start_date <= LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES) & (df.innings == inns)]\n",
    "    for inns in range(2)\n",
    "}\n",
    "df_test = {\n",
    "    inns: df[(df.start_date > LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES) & (df.innings == inns)]\n",
    "    for inns in range(2)\n",
    "}\n",
    "\n",
    "train_dfs = {\n",
    "    inns: {over: df_train[inns].loc[(df_train[inns][\"over\"] == over)] for over in range(20)} for inns in range(2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs[0][1].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(d, values: list[str]):\n",
    "    return d[values]\n",
    "\n",
    "\n",
    "X_values = [\n",
    "    \"wickets_down\",\n",
    "    \"run_rate\",\n",
    "    \"req_rate\",\n",
    "    \"batter_in_first_10\",\n",
    "    \"batter_strike_rate\",\n",
    "    \"batter_dismissal_prob\",\n",
    "    \"batter_dismissal_vs_style\",\n",
    "    \"bowler_economy\",\n",
    "    \"bowler_wicket_prob\",\n",
    "    \"bowler_wicket_vs _style\",\n",
    "    \"bowler_wide_noball_rate\",\n",
    "]\n",
    "\n",
    "y_values = [\n",
    "    \"outcome\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://towardsdatascience.com/tuning-with-hdbscan-149865ac2970\n",
    "\n",
    "joblib_memory = joblib.Memory()\n",
    "\n",
    "\n",
    "def optimize_hdb(X, param_dist: dict, inns: int, over: int):\n",
    "    hdb = hdbscan.HDBSCAN(\n",
    "        gen_min_span_tree=True,\n",
    "        memory=joblib_memory,\n",
    "        cluster_selection_method=\"leaf\",\n",
    "        metric=\"euclidean\",\n",
    "    ).fit(X)\n",
    "\n",
    "    scorer = make_scorer(hdbscan.validity.validity_index, greater_is_better=True)\n",
    "    # scorer = make_scorer(hdbscan.validity.validity_index, greater_is_better=True)\n",
    "\n",
    "    n_iter_search = 20\n",
    "    random_search = RandomizedSearchCV(\n",
    "        hdb, param_distributions=param_dist, n_iter=n_iter_search, scoring=scorer, random_state=1\n",
    "    )\n",
    "\n",
    "    random_search.fit(X)\n",
    "\n",
    "    print(f\"Best Parameters {random_search.best_params_}\")\n",
    "    dbcv_score = random_search.best_estimator_.relative_validity_\n",
    "    print(f\"DBCV score :{dbcv_score}\")\n",
    "\n",
    "    # evaluate the clusters\n",
    "    labels = random_search.best_estimator_.labels_\n",
    "    clustered = labels >= 0\n",
    "\n",
    "    coverage = np.sum(clustered) / X.shape[0]\n",
    "    total_clusters = np.max(labels) + 1\n",
    "    cluster_sizes = np.bincount(labels[clustered]).tolist()\n",
    "\n",
    "    print(f\"Percent of data retained: {coverage}\")\n",
    "    print(f\"Total Clusters found: {total_clusters}\")\n",
    "    print(f\"Cluster splits: {cluster_sizes}\")\n",
    "\n",
    "    best_params = [\n",
    "        random_search.best_params_[param]\n",
    "        for param in [\n",
    "            \"min_samples\",\n",
    "            \"min_cluster_size\",\n",
    "            \"cluster_selection_epsilon\",\n",
    "            \"alpha\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    str_vals = [\n",
    "        str(inns),\n",
    "        str(over),\n",
    "        str(best_params[0]),\n",
    "        str(best_params[1]),\n",
    "        str(best_params[2]),\n",
    "        str(best_params[3]),\n",
    "        f\"{dbcv_score:.5f}\",\n",
    "        f\"{coverage:.1%}\",\n",
    "        str(total_clusters),\n",
    "    ]\n",
    "\n",
    "    msg = \"| \" + \" | \".join(str_vals) + \" |\"\n",
    "\n",
    "    logger.info(msg.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"starting...\")\n",
    "for inns in range(1):\n",
    "    for over in range(2):\n",
    "        X = extract_values(train_dfs[inns][over], X_values)\n",
    "        # X = X.sample(1_000)\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        optimize_hdb(\n",
    "            X_scaled,\n",
    "            {\n",
    "                \"min_samples\": [5, 7, 10, 14, 20, 28, 36, 50, 70, 100],\n",
    "                \"min_cluster_size\": [50, 100, 140, 200, 280, 400],\n",
    "                \"cluster_selection_epsilon\": [0.1, 0.14, 0.2, 0.28, 0.4, 0.56, 0.7, 0.8, 0.9, 1.0, 1.1],\n",
    "                \"alpha\": [0.02, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0],\n",
    "            },\n",
    "            inns,\n",
    "            over,\n",
    "        )\n",
    "logger.info(\"all done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| over | min_samples | min_cluster_size | metric | cluster_selection_method | dbcv score | % data retained | clusters |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 0 | 60 | 50 | leaf | euclidean | 0.01718671973211158 | 0.3267860971484384 | 30 |\n",
    "| 1 | 60 | 50 | leaf | euclidean | 0.0004236590487102877 | 0.3814011337271879 | 21 |\n",
    "| 2 | 60 | 50 | leaf | euclidean | 0.0001322767377816463 | 0.49497887748943875 | 25 |\n",
    "| 3 | 60 | 50 | leaf | euclidean | 0.010659961352848637 | 0.5143344668594871 | 21 |\n",
    "| 4 | 60 | 50 | leaf | euclidean | 0.00010486661772658043 | 0.46883310867426864 | 18 |\n",
    "| 5 | 60 | 50 | leaf | euclidean | 0.00014236732093149724 | 0.33991540814958376 | 16 |\n",
    "| 6 | 60 | 50 | leaf | euclidean | 5.389679115965339e-05 | 0.2649577908493515 | 16 |\n",
    "| 7 | 60 | 50 | leaf | euclidean | 0.00024103239727212846 | 0.17515262364368867 | 16 |\n",
    "| 8 | 60 | 50 | leaf | euclidean | 0.14586455105206522 | 0.2875743170280636 | 13 |\n",
    "| 9 | 60 | 50 | leaf | euclidean | 0.16179595204736644 | 0.5197726026031018 | 11 |\n",
    "| 10 | 60 | 50 | leaf | euclidean | 0.0027864273693727357 | 0.6821199518989879 | 10 |\n",
    "| 11 | 60 | 50 | leaf | euclidean | 0.2569892575528506 | 0.6621440324009619 | 9 |\n",
    "| 12 | 60 | 50 | leaf | euclidean | 0.2213270535430393 | 0.6541379266380223 | 9 |\n",
    "| 13 | 60 | 50 | leaf | euclidean | 0.00023074855116647224 | 0.9396190402496744 | 8 |\n",
    "| 14 | 60 | 50 | leaf | euclidean | 0.00010857887839013185 | 0.9383274896645559 | 8 |\n",
    "| 15 | 60 | 50 | leaf | euclidean | 0.00016876378769864825 | 0.9377187219264961 | 7 |\n",
    "| 16 | 60 | 50 | leaf | euclidean | 0.005322336377586585 | 0.942445151427509 | 5 |\n",
    "| 17 | 60 | 50 | leaf | euclidean | 0.004093128615905713 | 0.9420499818906194 | 5 |\n",
    "| 18 | 60 | 50 | leaf | euclidean | 0.002438867408535001 | 0.9408895265423243 | 5 |\n",
    "| 19 | 60 | 50 | leaf | euclidean | 0.002230152927614161 | 0.9363866176877197 | 5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
