{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from app.utils import StopWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path.cwd().parent / \"logs\"\n",
    "logger.add(log_dir / \"log.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCHES = 250\n",
    "LAST_TRAIN_MATCH_DATE = datetime(2023, 7, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = Path().resolve()\n",
    "DATA_DIR = this_dir.parent / \"data\"\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / \"ml_rows.csv\", parse_dates=[\"start_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[(df.start_date <= LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES)]\n",
    "df_test = df[(df.start_date > LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES)]\n",
    "print(f\"{df_train.shape=}, {df_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_dfs = {over: df_train.loc[(df_train[\"over\"] == over)] for over in range(20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(d, over: int, values: list[str]):\n",
    "    return d[values]\n",
    "\n",
    "\n",
    "X_values = [\n",
    "    \"innings\",\n",
    "    \"ball_of_innings\",\n",
    "    \"wickets_down\",\n",
    "    \"run_rate\",\n",
    "    \"req_rate\",\n",
    "    \"batter_in_first_10\",\n",
    "    \"batter_strike_rate\",\n",
    "    \"bowler_economy\",\n",
    "    \"bowler_wicket_prob\",\n",
    "    \"bowler_wide_noball_rate\",\n",
    "]\n",
    "\n",
    "y_values = [\n",
    "    \"outcome\",\n",
    "]\n",
    "\n",
    "X_by_over = {over: extract_values(over_dfs[over], over, X_values) for over in range(20)}\n",
    "y_by_over = {over: extract_values(over_dfs[over], over, y_values) for over in range(20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_filename(name: str, over: int) -> str:\n",
    "    return f\"{name}_fitted_over_{over}.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Some Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "def fit_dbscan(\n",
    "    X_scaled, over: int, min_cluster_size: int, cluster_selection_method: str\n",
    ") -> tuple[int, int, int, int, float, float, int]:\n",
    "    clf = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "    )\n",
    "    clf.fit(X_scaled)\n",
    "    # joblib.dump(clf, DATA_DIR / dump_filename(f\"DBSCAN_{min_cluster_size}_{cluster_selection_method}\", over))\n",
    "    labels = clf.labels_\n",
    "    label_counts = np.unique(labels, return_counts=True)[1]\n",
    "    return (\n",
    "        len(labels[labels > -1]),\n",
    "        len(np.unique(clf.labels_)),\n",
    "        int(np.min(label_counts)),\n",
    "        int(np.max(label_counts)),\n",
    "        float(np.mean(label_counts)),\n",
    "        float(np.median(label_counts)),\n",
    "        len(labels[labels == -1]),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "X_scaled = {}\n",
    "with StopWatch(decimals=2) as stopwatch:\n",
    "    for over in range(0, 2):\n",
    "        X = X_by_over[over]\n",
    "        X = X.sample(10_000)\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X_scaled[over] = scaler.transform(X)\n",
    "        scalers[over] = scaler\n",
    "        # for cluster_selection_method in [\"eom\", \"leaf\"]:\n",
    "        for min_cluster_size in range(10, 101, 100):\n",
    "            labelled, labels, small, large, mean, median, noisy = fit_dbscan(\n",
    "                X_scaled[over], over, min_cluster_size, \"eom\"\n",
    "            )\n",
    "            row_count = labelled + noisy\n",
    "            msg = (\n",
    "                f\"\"\"over {over}/{min_cluster_size}: {labelled=} ({labelled / row_count:.1%}) {labels=} \"\"\"\n",
    "                f\"\"\"{small=} {large=} {mean=:.2f} {median=:.2f} {noisy=} ({noisy / row_count:.1%})\"\"\"\n",
    "            )\n",
    "            # stopwatch.report_split(msg)\n",
    "            logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hdb(X, param_dist: dict):\n",
    "    hdb = HDBSCAN(gen_min_span_tree=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_ranges = {\n",
    "    0: list(range(22, 39, 2)) + list(range(62, 79, 2)),\n",
    "    1: list(range(2, 19, 2)),\n",
    "    2: list(range(100)),\n",
    "    3: list(range(100)),\n",
    "    4: list(range(100)),\n",
    "    5: list(range(100)),\n",
    "    6: list(range(100)),\n",
    "    7: list(range(100)),\n",
    "    8: list(range(100)),\n",
    "    9: list(range(100)),\n",
    "    10: list(range(100)),\n",
    "    11: list(range(100)),\n",
    "    12: list(range(100)),\n",
    "    13: list(range(100)),\n",
    "    14: list(range(100)),\n",
    "    15: list(range(100)),\n",
    "    16: list(range(100)),\n",
    "    17: list(range(100)),\n",
    "    18: list(range(100)),\n",
    "    19: list(range(100)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def load_clf(over: int, clusters: int):\n",
    "    return joblib.load(DATA_DIR / clf_filename(over, clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "over = 0\n",
    "clusters = 25\n",
    "\n",
    "clf = load_clf(over, clusters)\n",
    "\n",
    "trained_predictions = clf.predict(X_scaled[over])\n",
    "actual_outcomes = y_by_over[over][\"outcome\"]\n",
    "print(trained_predictions.shape, actual_outcomes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cluster_preds = defaultdict(lambda: defaultdict(int))\n",
    "for pred, outcome in zip(trained_predictions, actual_outcomes):\n",
    "    cluster_preds[pred][outcome] += 1\n",
    "\n",
    "VALS = len(X_values)\n",
    "\n",
    "sums: dict[int, float] = {i: 0.0 for i in range(VALS)}\n",
    "grand_tot = 0\n",
    "\n",
    "for idx in range(clusters):\n",
    "    preds = cluster_preds[idx]\n",
    "    tot = sum(preds.values())\n",
    "    grand_tot += tot\n",
    "    pcts = [preds[idx] / tot for idx in range(VALS)]\n",
    "    for i in range(VALS):\n",
    "        sums[i] += preds[i]\n",
    "    print(f\"{idx:3d}\", f\"{tot:5d}\", \", \".join([f\"{v:6.2%}\" for v in pcts]))\n",
    "\n",
    "print(\"sums     \", \", \".join([f\"{v / grand_tot:6.2%}\" for v in sums.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test_over = df_test.loc[(df_test[\"over\"] == over)]\n",
    "X_test = extract_values(df_test_over, over, X_values)\n",
    "X_test_scaled = scalers[over].transform(X_test)\n",
    "test_predictions = clf.predict(X_test_scaled)\n",
    "y_actuals = extract_values(df_test_over, over, y_values)[\"outcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_preds = defaultdict(lambda: defaultdict(int))\n",
    "for pred, outcome in zip(test_predictions, y_actuals):\n",
    "    test_preds[pred][outcome] += 1\n",
    "\n",
    "VALS = len(X_values)\n",
    "\n",
    "\n",
    "sums: dict[int, float] = {i: 0.0 for i in range(VALS)}\n",
    "grand_tot = 0\n",
    "for idx in range(clusters):\n",
    "    preds = test_preds[idx]\n",
    "    tot = sum(preds.values())\n",
    "    grand_tot += tot\n",
    "    if tot > 0:\n",
    "        pcts = [preds[idx] / tot for idx in range(VALS)]\n",
    "        for i in range(VALS):\n",
    "            sums[i] += preds[i]\n",
    "\n",
    "        print(f\"{idx:3d}\", f\"{tot:5d}\", \", \".join([f\"{v:6.2%}\" for v in pcts]))\n",
    "\n",
    "print(\"sums     \", \", \".join([f\"{v / grand_tot:6.2%}\" for v in sums.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
