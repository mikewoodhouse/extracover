{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from app.utils import StopWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCHES = 1000\n",
    "LAST_TRAIN_MATCH_DATE = datetime(2023, 7, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = Path().resolve()\n",
    "DATA_DIR = this_dir.parent / \"data\"\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / \"ml_rows.csv\", parse_dates=[\"start_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[\n",
    "    (df.start_date <= LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES)\n",
    "]\n",
    "df_test = df[(df.start_date > LAST_TRAIN_MATCH_DATE) & (df.match_number > MIN_MATCHES)]\n",
    "print(f\"{df_train.shape=}, {df_test.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_dfs = {phase: df_train.loc[(df_train[\"phase\"] == phase)] for phase in [0, 1, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f\"{phase}: {phase_df.shape}\" for phase, phase_df in phase_dfs.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(d, phase: int, values: list[str]):\n",
    "    return d[values]\n",
    "\n",
    "\n",
    "X_values = [\n",
    "    \"phase\",\n",
    "    \"innings\",\n",
    "    \"ball_of_innings\",\n",
    "    \"wickets_down\",\n",
    "    \"run_rate\",\n",
    "    \"req_rate\",\n",
    "    \"batter_in_first_10\",\n",
    "    \"batter_strike_rate\",\n",
    "    \"bowler_economy\",\n",
    "    \"bowler_wicket_prob\",\n",
    "    \"bowler_wide_noball_rate\",\n",
    "]\n",
    "\n",
    "X_by_phase = {\n",
    "    phase: extract_values(phase_dfs[phase], phase, X_values) for phase in [0, 1, 2]\n",
    "}\n",
    "\n",
    "y_values = [\n",
    "    \"outcome\",\n",
    "]\n",
    "\n",
    "y_by_phase = {\n",
    "    phase: extract_values(phase_dfs[phase], phase, y_values) for phase in [0, 1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_filename(phase: int, clusters: int) -> str:\n",
    "    return f\"Kmeans_fitted_phase_{phase}_{clusters}_clusters.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans as Clustering\n",
    "\n",
    "\n",
    "def fit_phase(X_scaled, phase: int, clusters: int) -> float:\n",
    "    clf = Clustering(n_clusters=clusters)\n",
    "    clf.fit(X_scaled)\n",
    "    joblib.dump(clf, DATA_DIR / clf_filename(phase, clusters))\n",
    "    return clf.score(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Some Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "X_scaled = {}\n",
    "with StopWatch(decimals=2) as stopwatch:\n",
    "    for phase in [0, 1, 2]:\n",
    "        X = X_by_phase[phase]\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X_scaled[phase] = scaler.transform(X)\n",
    "        scalers[phase] = scaler\n",
    "        for clusters in [25, 50, 75, 100, 125, 150]:\n",
    "            score = fit_phase(X_scaled[phase], phase, clusters)\n",
    "            stopwatch.report_split(f\"fitted {phase=} with {clusters=} {score=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf(phase: int, clusters: int):\n",
    "    return joblib.load(DATA_DIR / clf_filename(phase, clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 0\n",
    "clusters = 25\n",
    "\n",
    "clf = load_clf(phase, clusters)\n",
    "\n",
    "trained_predictions = clf.predict(X_scaled[phase])\n",
    "actual_outcomes = y_by_phase[phase][\"outcome\"]\n",
    "print(trained_predictions.shape, actual_outcomes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cluster_preds = defaultdict(lambda: defaultdict(int))\n",
    "for pred, outcome in zip(trained_predictions, actual_outcomes):\n",
    "    cluster_preds[pred][outcome] += 1\n",
    "\n",
    "sums: dict[int, float] = {i: 0.0 for i in range(11)}\n",
    "grand_tot = 0\n",
    "\n",
    "for idx in range(clusters):\n",
    "    preds = cluster_preds[idx]\n",
    "    tot = sum(preds.values())\n",
    "    grand_tot += tot\n",
    "    pcts = [preds[idx] / tot for idx in range(11)]\n",
    "    for i in range(11):\n",
    "        sums[i] += preds[i]\n",
    "    print(f\"{idx:3d}\", f\"{tot:5d}\", \", \".join([f\"{v:6.2%}\" for v in pcts]))\n",
    "\n",
    "print(\"sums     \", \", \".join([f\"{v/grand_tot:6.2%}\" for v in sums.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_phase = df_test.loc[(df_test[\"phase\"] == phase)]\n",
    "X_test = extract_values(df_test_phase, phase, X_values)\n",
    "X_test_scaled = scalers[phase].transform(X_test)\n",
    "test_predictions = clf.predict(X_test_scaled)\n",
    "y_actuals = extract_values(df_test_phase, phase, y_values)[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = defaultdict(lambda: defaultdict(int))\n",
    "for pred, outcome in zip(test_predictions, y_actuals):\n",
    "    test_preds[pred][outcome] += 1\n",
    "\n",
    "sums: dict[int, float] = {i: 0.0 for i in range(11)}\n",
    "grand_tot = 0\n",
    "\n",
    "for idx in range(clusters):\n",
    "    preds = test_preds[idx]\n",
    "    tot = sum(preds.values())\n",
    "    grand_tot += tot\n",
    "    if tot > 0:\n",
    "        pcts = [preds[idx] / tot for idx in range(11)]\n",
    "        for i in range(11):\n",
    "            sums[i] += preds[i]\n",
    "\n",
    "        print(f\"{idx:3d}\", f\"{tot:5d}\", \", \".join([f\"{v:6.2%}\" for v in pcts]))\n",
    "\n",
    "print(\"sums     \", \", \".join([f\"{v/grand_tot:6.2%}\" for v in sums.values()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
